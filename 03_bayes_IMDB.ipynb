{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, ComplementNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import vstack, hstack\n",
    "\n",
    "import gc; gc.enable()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "\n",
    "ngram_min = 1\n",
    "ngram_max = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Embedding Files\n",
    "\n",
    "def read_embeddings(filename):\n",
    "    X=[]\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            vector = line.split()[1:]\n",
    "            vector = np.array(vector)\n",
    "            vector = vector.astype(np.float64)\n",
    "            X.append(vector)\n",
    "    return np.array(X)\n",
    "\n",
    "X_train_embedding = read_embeddings('emb_train.txt')\n",
    "X_test_embedding = read_embeddings('emb_test.txt')\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Docs\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "alldocs = []\n",
    "filename='alldata-3gram.txt'\n",
    "\n",
    "with open(filename, encoding='utf-8') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = line.split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] \n",
    "        split = ['train','test','extra','extra'][line_no//25000]  \n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] \n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "    \n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction for Naive Bayes Model\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer=lambda text: text,\n",
    "                             preprocessor=lambda text:text, \n",
    "                             binary=True,\n",
    "                             ngram_range=(ngram_min,ngram_max))\n",
    "\n",
    "X_train_NB = count_vect.fit_transform([x.words for x in train_docs])\n",
    "y_train = [doc.sentiment for doc in train_docs]\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bernoulli NB: \n",
    "    * https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html\n",
    "* Multi-Nomial NB: \n",
    "    * https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\n",
    "* Complement NB: \n",
    "    * https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html#sklearn.naive_bayes.ComplementNB\n",
    "* Log-Probability:\n",
    "    * https://en.wikipedia.org/wiki/Log_probability\n",
    "* Log-Odds:\n",
    "    * https://wiki.lesswrong.com/wiki/Log_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities / Odds\n",
    "\n",
    "# nb = BernoulliNB()\n",
    "# nb = MultinomialNB()\n",
    "nb = ComplementNB()\n",
    "\n",
    "nb.fit(X_train_NB, y_train)\n",
    "prob = nb.feature_log_prob_ \n",
    "\n",
    "# log(p)-log(q) = log(p/q) \n",
    "#               = log(p/(1-p))\n",
    "#               = log(odds)\n",
    "log_odds = prob[0] - prob[1]\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Adjustments\n",
    "\n",
    "X_train = [x.multiply(log_odds).tocsr() for x in X_train_NB]\n",
    "X_train = vstack(X_train)\n",
    "\n",
    "X_test_NB = count_vect.transform([x.words for x in test_docs])\n",
    "X_test = [x.multiply(log_odds).tocsr() for x in X_test_NB]\n",
    "X_test = vstack(X_test)\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "Accuracy = 100.0\n",
      "\n",
      "TEST:\n",
      "Accuracy = 93.43599999999999\n"
     ]
    }
   ],
   "source": [
    "# Train / Predict / Score\n",
    "\n",
    "X_train = hstack((X_train, X_train_embedding))\n",
    "X_test = hstack((X_test, X_test_embedding))\n",
    "\n",
    "print('TRAIN:')\n",
    "clf = LogisticRegression(n_jobs=-1, \n",
    "                         class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "print('Accuracy =', clf.score(X_train, y_train)*100)\n",
    "\n",
    "print()\n",
    "print('TEST:')\n",
    "y_test = [doc.sentiment for doc in test_docs]\n",
    "print('Accuracy =', clf.score(X_test, y_test)*100)\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, \n",
    "                         X_train, y_train, \n",
    "                         scoring='accuracy', \n",
    "                         cv=5)\n",
    "\n",
    "print(scores.mean(), \"+/-\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.947 , 0.9444, 0.9466, 0.9508, 0.9526])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
