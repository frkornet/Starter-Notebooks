{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning\n",
    "\n",
    "Here is a simple implementation of DeepMind's AlphaZero algorithm for Connect 4.\n",
    "\n",
    "See [this blog post](https://towardsdatascience.com/from-scratch-implementation-of-alphazero-for-connect4-f73d4554002a) for more information.\n",
    "\n",
    "The AlphaZero architecture is probably overkill for this problem (there are 19 residual blocks consisting multiple hidden layers). I recommend stealing these ideas and simplifying the architecture.\n",
    "\n",
    "## $Q$-Learning\n",
    "\n",
    "Before we look at the code, let's get a general sense of the algorithm. AlphaZero is utilizing a form of $Q$-learning.\n",
    "\n",
    "See [this Wiki](https://en.wikipedia.org/wiki/Q-learning).\n",
    "\n",
    "I like to think about $Q$-learning with a classic text-based adventure analogy. Here's a good [example](https://www.kongregate.com/games/rete/dont-shit-your-pants).\n",
    "\n",
    "How does this work? There are different states a player might find themselves in. For each state, there is an optimal *move* (action) which gets you closer to a desired state (winning the game / minimizing your loss). We would like to know what the optimal function $Q$ is, where $Q(\\textrm{State}) = \\textrm{Action}$. This function $Q$ is known as the **Policy** and this is what we want to learn.\n",
    "\n",
    "But how is AlphaZero doing this specifically? To answer that question fully, you probably want to familiarize yourself with [AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far) first. But [here](https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go) is a brief overview with links to the technical articles and other resources.\n",
    "\n",
    "## AlphaGo - The algorithm that changed everything\n",
    "\n",
    "If you have not seen the AlphaGo documentary, I recommend it: https://www.alphagomovie.com/\n",
    "\n",
    "# AlphaZero for Connect 4\n",
    "\n",
    "Let's get into it$\\dots$\n",
    "\n",
    "Again, the original implementation and description can be found [here](https://towardsdatascience.com/from-scratch-implementation-of-alphazero-for-connect4-f73d4554002a).\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Basic\n",
    "import collections\n",
    "import math\n",
    "import copy\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visual\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from matplotlib.table import Table\n",
    "\n",
    "# NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Board Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class board():\n",
    "    def __init__(self):\n",
    "        self.init_board = np.zeros([6,7]).astype(str)\n",
    "        self.init_board[self.init_board == \"0.0\"] = \" \"\n",
    "        self.player = 0\n",
    "        self.current_board = self.init_board\n",
    "        \n",
    "    def drop_piece(self, column):\n",
    "        if self.current_board[0, column] != \" \":\n",
    "            return \"Invalid move\"\n",
    "        else:\n",
    "            row = 0; pos = \" \"\n",
    "            while (pos == \" \"):\n",
    "                if row == 6:\n",
    "                    row += 1\n",
    "                    break\n",
    "                pos = self.current_board[row, column]\n",
    "                row += 1\n",
    "            if self.player == 0:\n",
    "                self.current_board[row-2, column] = \"O\"\n",
    "                self.player = 1\n",
    "            elif self.player == 1:\n",
    "                self.current_board[row-2, column] = \"X\"\n",
    "                self.player = 0\n",
    "    \n",
    "    def check_winner(self):\n",
    "        if self.player == 1:\n",
    "            for row in range(6):\n",
    "                for col in range(7):\n",
    "                    if self.current_board[row, col] != \" \":\n",
    "                        # rows\n",
    "                        try:\n",
    "                            if self.current_board[row, col] == \"O\" and self.current_board[row + 1, col] == \"O\" and \\\n",
    "                                self.current_board[row + 2, col] == \"O\" and self.current_board[row + 3, col] == \"O\":\n",
    "                                #print(\"row\")\n",
    "                                return True\n",
    "                        except IndexError:\n",
    "                            next\n",
    "                        # columns\n",
    "                        try:\n",
    "                            if self.current_board[row, col] == \"O\" and self.current_board[row, col + 1] == \"O\" and \\\n",
    "                                self.current_board[row, col + 2] == \"O\" and self.current_board[row, col + 3] == \"O\":\n",
    "                                #print(\"col\")\n",
    "                                return True\n",
    "                        except IndexError:\n",
    "                            next\n",
    "                        # \\ diagonal\n",
    "                        try:\n",
    "                            if self.current_board[row, col] == \"O\" and self.current_board[row + 1, col + 1] == \"O\" and \\\n",
    "                                self.current_board[row + 2, col + 2] == \"O\" and self.current_board[row + 3, col + 3] == \"O\":\n",
    "                                #print(\"\\\\\")\n",
    "                                return True\n",
    "                        except IndexError:\n",
    "                            next\n",
    "                        # / diagonal\n",
    "                        try:\n",
    "                            if self.current_board[row, col] == \"O\" and self.current_board[row + 1, col - 1] == \"O\" and \\\n",
    "                                self.current_board[row + 2, col - 2] == \"O\" and self.current_board[row + 3, col - 3] == \"O\"\\\n",
    "                                and (col-3) >= 0:\n",
    "                                #print(\"/\")\n",
    "                                return True\n",
    "                        except IndexError:\n",
    "                            next\n",
    "        if self.player == 0:\n",
    "            for row in range(6):\n",
    "                for col in range(7):\n",
    "                    if self.current_board[row, col] != \" \":\n",
    "                        # rows\n",
    "                        try:\n",
    "                            if self.current_board[row, col] == \"X\" and self.current_board[row + 1, col] == \"X\" and \\\n",
    "                                self.current_board[row + 2, col] == \"X\" and self.current_board[row + 3, col] == \"X\":\n",
    "                                return True\n",
    "                        except IndexError:\n",
    "                            next\n",
    "                        # columns\n",
    "                        try:\n",
    "                            if self.current_board[row, col] == \"X\" and self.current_board[row, col + 1] == \"X\" and \\\n",
    "                                self.current_board[row, col + 2] == \"X\" and self.current_board[row, col + 3] == \"X\":\n",
    "                                return True\n",
    "                        except IndexError:\n",
    "                            next\n",
    "                        # \\ diagonal\n",
    "                        try:\n",
    "                            if self.current_board[row, col] == \"X\" and self.current_board[row + 1, col + 1] == \"X\" and \\\n",
    "                                self.current_board[row + 2, col + 2] == \"X\" and self.current_board[row + 3, col + 3] == \"X\":\n",
    "                                return True\n",
    "                        except IndexError:\n",
    "                            next\n",
    "                        # / diagonal\n",
    "                        try:\n",
    "                            if self.current_board[row, col] == \"X\" and self.current_board[row + 1, col - 1] == \"X\" and \\\n",
    "                                self.current_board[row + 2, col - 2] == \"X\" and self.current_board[row + 3, col - 3] == \"X\"\\\n",
    "                                and (col-3) >= 0:\n",
    "                                return True\n",
    "                        except IndexError:\n",
    "                            next\n",
    "    def actions(self): # returns all possible moves\n",
    "        acts = []\n",
    "        for col in range(7):\n",
    "            if self.current_board[0, col] == \" \":\n",
    "                acts.append(col)\n",
    "        return acts\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization - Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_board(np_data, fmt='{:s}', bkg_colors=['pink', 'pink']):\n",
    "    data = pd.DataFrame(np_data, columns=['0','1','2','3','4','5','6'])\n",
    "    fig, ax = plt.subplots(figsize=[7,7])\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax, bbox=[0,0,1,1])\n",
    "    nrows, ncols = data.shape\n",
    "    width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "    for (i,j), val in np.ndenumerate(data):\n",
    "        idx = [j % 2, (j + 1) % 2][i % 2]\n",
    "        color = bkg_colors[idx]\n",
    "\n",
    "        tb.add_cell(i, j, width, height, text=fmt.format(val), \n",
    "                    loc='center', facecolor=color)\n",
    "\n",
    "    for i, label in enumerate(data.index):\n",
    "        tb.add_cell(i, -1, width, height, text=label, loc='right', \n",
    "                    edgecolor='none', facecolor='none')\n",
    "\n",
    "    for j, label in enumerate(data.columns):\n",
    "        tb.add_cell(-1, j, width, height/2, text=label, loc='center', \n",
    "                           edgecolor='none', facecolor='none')\n",
    "    tb.set_fontsize(24)\n",
    "    ax.add_table(tb)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = board()\n",
    "game.drop_piece(0)\n",
    "game.drop_piece(1)\n",
    "game.drop_piece(1)\n",
    "state = game.current_board\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_board(state)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaZero Ingredients\n",
    "\n",
    "1. $\\textrm{Board State} \\mapsto \\textrm{Policy, Value}$\n",
    "    * Policy: Probability Distribution of Possible Future Moves\n",
    "    * Value: Prediction of Game Outcome given Current State\n",
    "        * White Wins: +1, Draw: 0, Black Wins: -1\n",
    "    * This is accomplished with a neural net.\n",
    "2. Think $n$-moves ahead.\n",
    "    * This is accomplished with a limited tree search algorithm.\n",
    "3. Learning from self-play.\n",
    "    * This is accomplished by survival of the fittest.\n",
    "\n",
    "### Deep Convolutional Residual Neural Net\n",
    "\n",
    "* 1 Convolutional Block:\n",
    "    * 128 filters ($3\\times 3$ kernel, stride 1)\n",
    "    * Batch Norm + ReLU\n",
    "* 19 Residual Blocks:\n",
    "    * 128 filters ($3\\times 3$ kernel, stride 1)\n",
    "    * Batch Norm + ReLU\n",
    "    * 128 filters ($3\\times 3$ kernel, stride 1)\n",
    "    * Batch Norm + **Residual Connection** + ReLU\n",
    "* 1 Output BLock:\n",
    "    * Policy: \n",
    "        * Convolution of 32 filters ($1\\times 1$, stride 1)\n",
    "        * Batch Norm + ReLU + Linear + SoftMax\n",
    "    * Value:\n",
    "        * Convolution of 3 filters ($1\\times 1$, stride 1)\n",
    "        * Batch Norm + ReLU + Linear + ReLU + Linear + Tanh\n",
    "\n",
    "### Monte-Carlo Tree Search\n",
    "\n",
    "The neural networks policy distribution helps guide the tree search.\n",
    "\n",
    "A game can be thought of as a tree in which the root is the initial state of the game and each child-branch is a possible state of continuation of play from a parent-state. However, it is impractical (and in certain games, impossible) to simply brute-force search all game states for the best possible line of play. We need to optimize the Exploration-Exploitation Tradeoff. This is done in AlphaZero by using a MCTS.\n",
    "\n",
    "### Self-Play Evaluation\n",
    "\n",
    "After one iteration (of an epoch checkpoint) in which the neural net is trained using MCTS self-play data, this trained neural net is then pitted against its previous version, again using MCTS guided by the respective neural net. The neural network that performs better (eg. Wins the majority of games) would then be used for the next iteration. This ensures that the net is always improving.\n",
    "\n",
    "-----\n",
    "\n",
    "## Board Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class board_data(Dataset):\n",
    "    def __init__(self, dataset): # dataset = np.array of (s, p, v)\n",
    "        self.X = dataset[:,0]\n",
    "        self.y_p, self.y_v = dataset[:,1], dataset[:,2]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return np.int64(self.X[idx].transpose(2,0,1)), self.y_p[idx], self.y_v[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvBlock Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.action_size = 7\n",
    "        self.conv1 = nn.Conv2d(3, 128, 3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "\n",
    "    def forward(self, s):\n",
    "        s = s.view(-1, 3, 6, 7)  # batch_size x channels x board_x x board_y\n",
    "        s = F.relu(self.bn1(self.conv1(s)))\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResBlock Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, inplanes=128, planes=128, stride=1, downsample=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OutBlock Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OutBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(128, 3, kernel_size=1) # value head\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        self.fc1 = nn.Linear(3*6*7, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(128, 32, kernel_size=1) # policy head\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.fc = nn.Linear(6*7*32, 7)\n",
    "    \n",
    "    def forward(self,s):\n",
    "        v = F.relu(self.bn(self.conv(s))) # value head\n",
    "        v = v.view(-1, 3*6*7)  # batch_size X channel X height X width\n",
    "        v = F.relu(self.fc1(v))\n",
    "        v = torch.tanh(self.fc2(v))\n",
    "        \n",
    "        p = F.relu(self.bn1(self.conv1(s))) # policy head\n",
    "        p = p.view(-1, 6*7*32)\n",
    "        p = self.fc(p)\n",
    "        p = self.logsoftmax(p).exp()\n",
    "        return p, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConnectNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConnectNet, self).__init__()\n",
    "        self.conv = ConvBlock()\n",
    "        for block in range(19):\n",
    "            setattr(self, \"res_%i\" % block,ResBlock())\n",
    "        self.outblock = OutBlock()\n",
    "    \n",
    "    def forward(self,s):\n",
    "        s = self.conv(s)\n",
    "        for block in range(19):\n",
    "            s = getattr(self, \"res_%i\" % block)(s)\n",
    "        s = self.outblock(s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaLoss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlphaLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_value, value, y_policy, policy):\n",
    "        value_error = (value - y_value) ** 2\n",
    "        policy_error = torch.sum((-policy* \n",
    "                                (1e-8 + y_policy.float()).float().log()), 1)\n",
    "        total_error = (value_error.view(-1).float() + policy_error).mean()\n",
    "        return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / Load - Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pickle(filename, data):\n",
    "    completeName = os.path.join(\"./datasets/\",\\\n",
    "                                filename)\n",
    "    with open(completeName, 'wb') as output:\n",
    "        pickle.dump(data, output)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    completeName = os.path.join(\"./datasets/\",\\\n",
    "                                filename)\n",
    "    with open(completeName, 'rb') as pkl_file:\n",
    "        data = pickle.load(pkl_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCTNode Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCTNode():\n",
    "    def __init__(self, game, move, parent=None):\n",
    "        self.game = game # state s\n",
    "        self.move = move # action index\n",
    "        self.is_expanded = False\n",
    "        self.parent = parent  \n",
    "        self.children = {}\n",
    "        self.child_priors = np.zeros([7], dtype=np.float32)\n",
    "        self.child_total_value = np.zeros([7], dtype=np.float32)\n",
    "        self.child_number_visits = np.zeros([7], dtype=np.float32)\n",
    "        self.action_idxes = []\n",
    "        \n",
    "    @property\n",
    "    def number_visits(self):\n",
    "        return self.parent.child_number_visits[self.move]\n",
    "\n",
    "    @number_visits.setter\n",
    "    def number_visits(self, value):\n",
    "        self.parent.child_number_visits[self.move] = value\n",
    "    \n",
    "    @property\n",
    "    def total_value(self):\n",
    "        return self.parent.child_total_value[self.move]\n",
    "    \n",
    "    @total_value.setter\n",
    "    def total_value(self, value):\n",
    "        self.parent.child_total_value[self.move] = value\n",
    "    \n",
    "    def child_Q(self):\n",
    "        return self.child_total_value / (1 + self.child_number_visits)\n",
    "    \n",
    "    def child_U(self):\n",
    "        return math.sqrt(self.number_visits) * (\n",
    "            abs(self.child_priors) / (1 + self.child_number_visits))\n",
    "    \n",
    "    def best_child(self):\n",
    "        if self.action_idxes != []:\n",
    "            bestmove = self.child_Q() + self.child_U()\n",
    "            bestmove = self.action_idxes[np.argmax(bestmove[self.action_idxes])]\n",
    "        else:\n",
    "            bestmove = np.argmax(self.child_Q() + self.child_U())\n",
    "        return bestmove\n",
    "    \n",
    "    def select_leaf(self):\n",
    "        current = self\n",
    "        while current.is_expanded:\n",
    "          best_move = current.best_child()\n",
    "          current = current.maybe_add_child(best_move)\n",
    "        return current\n",
    "    \n",
    "    def add_dirichlet_noise(self,action_idxs,child_priors):\n",
    "        valid_child_priors = child_priors[action_idxs] # select only legal moves entries in child_priors array\n",
    "        valid_child_priors = 0.75*valid_child_priors + 0.25*np.random.dirichlet(np.zeros([len(valid_child_priors)], \\\n",
    "                                                                                          dtype=np.float32)+192)\n",
    "        child_priors[action_idxs] = valid_child_priors\n",
    "        return child_priors\n",
    "    \n",
    "    def expand(self, child_priors):\n",
    "        self.is_expanded = True\n",
    "        action_idxs = self.game.actions(); c_p = child_priors\n",
    "        if action_idxs == []:\n",
    "            self.is_expanded = False\n",
    "        self.action_idxes = action_idxs\n",
    "        c_p[[i for i in range(len(child_priors)) if i not in action_idxs]] = 0.000000000 # mask all illegal actions\n",
    "        if self.parent.parent == None: # add dirichlet noise to child_priors in root node\n",
    "            c_p = self.add_dirichlet_noise(action_idxs,c_p)\n",
    "        self.child_priors = c_p\n",
    "    \n",
    "    def decode_n_move_pieces(self,board,move):\n",
    "        board.drop_piece(move)\n",
    "        return board\n",
    "            \n",
    "    def maybe_add_child(self, move):\n",
    "        if move not in self.children:\n",
    "            copy_board = copy.deepcopy(self.game) # make copy of board\n",
    "            copy_board = self.decode_n_move_pieces(copy_board,move)\n",
    "            self.children[move] = UCTNode(\n",
    "              copy_board, move, parent=self)\n",
    "        return self.children[move]\n",
    "    \n",
    "    def backup(self, value_estimate: float):\n",
    "        current = self\n",
    "        while current.parent is not None:\n",
    "            current.number_visits += 1\n",
    "            if current.game.player == 1: # same as current.parent.game.player = 0\n",
    "                current.total_value += (1*value_estimate) # value estimate +1 = O wins\n",
    "            elif current.game.player == 0: # same as current.parent.game.player = 1\n",
    "                current.total_value += (-1*value_estimate)\n",
    "            current = current.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS_c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyNode(object):\n",
    "    def __init__(self):\n",
    "        self.parent = None\n",
    "        self.child_total_value = collections.defaultdict(float)\n",
    "        self.child_number_visits = collections.defaultdict(float)\n",
    "\n",
    "def UCT_search(game_state, num_reads,net,temp):\n",
    "    root = UCTNode(game_state, move=None, parent=DummyNode())\n",
    "    for i in range(num_reads):\n",
    "        leaf = root.select_leaf()\n",
    "        encoded_s = encode_board(leaf.game); encoded_s = encoded_s.transpose(2,0,1)\n",
    "        encoded_s = torch.from_numpy(encoded_s).float().cuda()\n",
    "        child_priors, value_estimate = net(encoded_s)\n",
    "        child_priors = child_priors.detach().cpu().numpy().reshape(-1); value_estimate = value_estimate.item()\n",
    "        if leaf.game.check_winner() == True or leaf.game.actions() == []: # if somebody won or draw\n",
    "            leaf.backup(value_estimate); continue\n",
    "        leaf.expand(child_priors) # need to make sure valid moves\n",
    "        leaf.backup(value_estimate)\n",
    "    return root\n",
    "\n",
    "def do_decode_n_move_pieces(board,move):\n",
    "    board.drop_piece(move)\n",
    "    return board\n",
    "\n",
    "def get_policy(root, temp=1):\n",
    "    #policy = np.zeros([7], dtype=np.float32)\n",
    "    #for idx in np.where(root.child_number_visits!=0)[0]:\n",
    "    #    policy[idx] = ((root.child_number_visits[idx])**(1/temp))/sum(root.child_number_visits**(1/temp))\n",
    "    return ((root.child_number_visits)**(1/temp))/sum(root.child_number_visits**(1/temp))\n",
    "\n",
    "def MCTS_self_play(connectnet, num_games, start_idx, cpu, args, iteration):\n",
    "    \n",
    "    if not os.path.isdir(\"./datasets/iter_%d\" % iteration):\n",
    "        if not os.path.isdir(\"datasets\"):\n",
    "            os.mkdir(\"datasets\")\n",
    "        os.mkdir(\"datasets/iter_%d\" % iteration)\n",
    "        \n",
    "    for idxx in tqdm(range(start_idx, num_games + start_idx)):\n",
    "        current_board = board()\n",
    "        checkmate = False\n",
    "        dataset = [] # to get state, policy, value for neural network training\n",
    "        states = []\n",
    "        value = 0\n",
    "        move_count = 0\n",
    "        while checkmate == False and current_board.actions() != []:\n",
    "            if move_count < 11:\n",
    "                t = args.temperature_MCTS\n",
    "            else:\n",
    "                t = 0.1\n",
    "            states.append(copy.deepcopy(current_board.current_board))\n",
    "            board_state = copy.deepcopy(encode_board(current_board))\n",
    "            root = UCT_search(current_board,777,connectnet,t)\n",
    "            policy = get_policy(root, t); print(\"[CPU: %d]: Game %d POLICY:\\n \" % (cpu, idxx), policy)\n",
    "            current_board = do_decode_n_move_pieces(current_board,\\\n",
    "                                                    np.random.choice(np.array([0,1,2,3,4,5,6]), \\\n",
    "                                                                     p = policy)) # decode move and move piece(s)\n",
    "            dataset.append([board_state,policy])\n",
    "            print(\"[Iteration: %d CPU: %d]: Game %d CURRENT BOARD:\\n\" % (iteration, cpu, idxx), current_board.current_board,current_board.player); print(\" \")\n",
    "            if current_board.check_winner() == True: # if somebody won\n",
    "                if current_board.player == 0: # black wins\n",
    "                    value = -1\n",
    "                elif current_board.player == 1: # white wins\n",
    "                    value = 1\n",
    "                checkmate = True\n",
    "            move_count += 1\n",
    "        dataset_p = []\n",
    "        for idx,data in enumerate(dataset):\n",
    "            s,p = data\n",
    "            if idx == 0:\n",
    "                dataset_p.append([s,p,0])\n",
    "            else:\n",
    "                dataset_p.append([s,p,value])\n",
    "        del dataset\n",
    "        save_as_pickle(\"iter_%d/\" % iteration +\\\n",
    "                       \"dataset_iter%d_cpu%i_%i_%s\" % (iteration, cpu, idxx, datetime.datetime.today().strftime(\"%Y-%m-%d\")), dataset_p)\n",
    "\n",
    "def run_MCTS(args, start_idx=0, iteration=0):\n",
    "    net_to_play=\"%s_iter%d.pth.tar\" % (args.neural_net_name, iteration)\n",
    "    net = ConnectNet()\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    if args.MCTS_num_processes > 1:\n",
    "        mp.set_start_method(\"spawn\",force=True)\n",
    "        net.share_memory()\n",
    "        net.eval()\n",
    "    \n",
    "        current_net_filename = os.path.join(\"./model_data/\",\\\n",
    "                                        net_to_play)\n",
    "        if os.path.isfile(current_net_filename):\n",
    "            checkpoint = torch.load(current_net_filename)\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "        else:\n",
    "            torch.save({'state_dict': net.state_dict()}, os.path.join(\"./model_data/\",\\\n",
    "                        net_to_play))\n",
    "        \n",
    "        processes = []\n",
    "        if args.MCTS_num_processes > mp.cpu_count():\n",
    "            num_processes = mp.cpu_count()\n",
    "        else:\n",
    "            num_processes = args.MCTS_num_processes\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(num_processes):\n",
    "                p = mp.Process(target=MCTS_self_play, args=(net, args.num_games_per_MCTS_process, start_idx, i, args, iteration))\n",
    "                p.start()\n",
    "                processes.append(p)\n",
    "            for p in processes:\n",
    "                p.join()\n",
    "    \n",
    "    elif args.MCTS_num_processes == 1:\n",
    "        net.eval()\n",
    "        \n",
    "        current_net_filename = os.path.join(\"./model_data/\",\\\n",
    "                                        net_to_play)\n",
    "        if os.path.isfile(current_net_filename):\n",
    "            checkpoint = torch.load(current_net_filename)\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "        else:\n",
    "            torch.save({'state_dict': net.state_dict()}, os.path.join(\"./model_data/\",\\\n",
    "                        net_to_play))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            MCTS_self_play(net, args.num_games_per_MCTS_process, start_idx, 0, args, iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder / Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_board(board):\n",
    "    board_state = board.current_board\n",
    "    encoded = np.zeros([6,7,3]).astype(int)\n",
    "    encoder_dict = {\"O\":0, \"X\":1}\n",
    "    for row in range(6):\n",
    "        for col in range(7):\n",
    "            if board_state[row,col] != \" \":\n",
    "                encoded[row, col, encoder_dict[board_state[row,col]]] = 1\n",
    "    if board.player == 1:\n",
    "        encoded[:,:,2] = 1 # player to move\n",
    "    return encoded\n",
    "\n",
    "def decode_board(encoded):\n",
    "    decoded = np.zeros([6,7]).astype(str)\n",
    "    decoded[decoded == \"0.0\"] = \" \"\n",
    "    decoder_dict = {0:\"O\", 1:\"X\"}\n",
    "    for row in range(6):\n",
    "        for col in range(7):\n",
    "            for k in range(2):\n",
    "                if encoded[row, col, k] == 1:\n",
    "                    decoded[row, col] = decoder_dict[k]\n",
    "    cboard = board()\n",
    "    cboard.current_board = decoded\n",
    "    cboard.player = encoded[0,0,2]\n",
    "    return cboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arena():\n",
    "    def __init__(self, current_cnet, best_cnet):\n",
    "        self.current = current_cnet\n",
    "        self.best = best_cnet\n",
    "    \n",
    "    def play_round(self):\n",
    "        if np.random.uniform(0,1) <= 0.5:\n",
    "            white = self.current; black = self.best; w = \"current\"; b = \"best\"\n",
    "        else:\n",
    "            white = self.best; black = self.current; w = \"best\"; b = \"current\"\n",
    "        current_board = board()\n",
    "        checkmate = False\n",
    "        dataset = []\n",
    "        value = 0; t = 0.1\n",
    "        while checkmate == False and current_board.actions() != []:\n",
    "            dataset.append(copy.deepcopy(encode_board(current_board)))\n",
    "            print(\"\"); print(current_board.current_board)\n",
    "            if current_board.player == 0:\n",
    "                root = UCT_search(current_board,777,white,t)\n",
    "                policy = get_policy(root, t); print(\"Policy: \", policy, \"white = %s\" %(str(w)))\n",
    "            elif current_board.player == 1:\n",
    "                root = UCT_search(current_board,777,black,t)\n",
    "                policy = get_policy(root, t); print(\"Policy: \", policy, \"black = %s\" %(str(b)))\n",
    "            current_board = do_decode_n_move_pieces(current_board,\\\n",
    "                                                    np.random.choice(np.array([0,1,2,3,4,5,6]), \\\n",
    "                                                                     p = policy)) # decode move and move piece(s)\n",
    "            if current_board.check_winner() == True: # someone wins\n",
    "                if current_board.player == 0: # black wins\n",
    "                    value = -1\n",
    "                elif current_board.player == 1: # white wins\n",
    "                    value = 1\n",
    "                checkmate = True\n",
    "        dataset.append(encode_board(current_board))\n",
    "        if value == -1:\n",
    "            dataset.append(f\"{b} as black wins\")\n",
    "            return b, dataset\n",
    "        elif value == 1:\n",
    "            dataset.append(f\"{w} as white wins\")\n",
    "            return w, dataset\n",
    "        else:\n",
    "            dataset.append(\"Nobody wins\")\n",
    "            return None, dataset\n",
    "    \n",
    "    def evaluate(self, num_games, cpu):\n",
    "        current_wins = 0\n",
    "        for i in range(num_games):\n",
    "            with torch.no_grad():\n",
    "                winner, dataset = self.play_round(); print(\"%s wins!\" % winner)\n",
    "            if winner == \"current\":\n",
    "                current_wins += 1\n",
    "            save_as_pickle(\"evaluate_net_dataset_cpu%i_%i_%s_%s\" % (cpu,i,datetime.datetime.today().strftime(\"%Y-%m-%d\"),\\\n",
    "                                                                     str(winner)),dataset)\n",
    "        print(\"Current_net wins ratio: %.5f\" % (current_wins/num_games))\n",
    "        save_as_pickle(\"wins_cpu_%i\" % (cpu),\\\n",
    "                                             {\"best_win_ratio\": current_wins/num_games, \"num_games\":num_games})\n",
    "        \n",
    "def fork_process(arena_obj, num_games, cpu): # make arena picklable\n",
    "    arena_obj.evaluate(num_games, cpu)\n",
    "\n",
    "def evaluate_nets(args, iteration_1, iteration_2) :\n",
    "    current_net=\"%s_iter%d.pth.tar\" % (args.neural_net_name, iteration_2); best_net=\"%s_iter%d.pth.tar\" % (args.neural_net_name, iteration_1)\n",
    "    current_net_filename = os.path.join(\"./model_data/\",\\\n",
    "                                    current_net)\n",
    "    best_net_filename = os.path.join(\"./model_data/\",\\\n",
    "                                    best_net)\n",
    "    \n",
    "    current_cnet = ConnectNet()\n",
    "    best_cnet = ConnectNet()\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        current_cnet.cuda()\n",
    "        best_cnet.cuda()\n",
    "    \n",
    "    if not os.path.isdir(\"./evaluator_data/\"):\n",
    "        os.mkdir(\"evaluator_data\")\n",
    "    \n",
    "    if args.MCTS_num_processes > 1:\n",
    "        mp.set_start_method(\"spawn\",force=True)\n",
    "        \n",
    "        current_cnet.share_memory(); best_cnet.share_memory()\n",
    "        current_cnet.eval(); best_cnet.eval()\n",
    "        \n",
    "        checkpoint = torch.load(current_net_filename)\n",
    "        current_cnet.load_state_dict(checkpoint['state_dict'])\n",
    "        checkpoint = torch.load(best_net_filename)\n",
    "        best_cnet.load_state_dict(checkpoint['state_dict'])\n",
    "         \n",
    "        processes = []\n",
    "        if args.MCTS_num_processes > mp.cpu_count():\n",
    "            num_processes = mp.cpu_count()\n",
    "        else:\n",
    "            num_processes = args.MCTS_num_processes\n",
    "        with torch.no_grad():\n",
    "            for i in range(num_processes):\n",
    "                p = mp.Process(target=fork_process,args=(arena(current_cnet,best_cnet), args.num_evaluator_games, i))\n",
    "                p.start()\n",
    "                processes.append(p)\n",
    "            for p in processes:\n",
    "                p.join()\n",
    "               \n",
    "        wins_ratio = 0.0\n",
    "        for i in range(num_processes):\n",
    "            stats = load_pickle(\"wins_cpu_%i\" % (i))\n",
    "            wins_ratio += stats['best_win_ratio']\n",
    "        wins_ratio = wins_ratio/num_processes\n",
    "        if wins_ratio >= 0.55:\n",
    "            return iteration_2\n",
    "        else:\n",
    "            return iteration_1\n",
    "            \n",
    "    elif args.MCTS_num_processes == 1:\n",
    "        current_cnet.eval(); best_cnet.eval()\n",
    "        checkpoint = torch.load(current_net_filename)\n",
    "        current_cnet.load_state_dict(checkpoint['state_dict'])\n",
    "        checkpoint = torch.load(best_net_filename)\n",
    "        best_cnet.load_state_dict(checkpoint['state_dict'])\n",
    "        arena1 = arena(current_cnet=current_cnet, best_cnet=best_cnet)\n",
    "        arena1.evaluate(num_games=args.num_evaluator_games, cpu=0)\n",
    "        \n",
    "        stats = load_pickle(\"wins_cpu_%i\" % (0))\n",
    "        if stats.best_win_ratio >= 0.55:\n",
    "            return iteration_2\n",
    "        else:\n",
    "            return iteration_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Against the Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def play_game(net):\n",
    "#     # Asks human what he/she wanna play as\n",
    "#     white = None; black = None\n",
    "#     while (True):\n",
    "#         play_as = input(\"What do you wanna play as? (\\\"O\\\"/\\\"X\\\")? Note: \\\"O\\\" starts first, \\\"X\\\" starts second\\n\")\n",
    "#         if play_as == \"O\":\n",
    "#             black = net; break\n",
    "#         elif play_as == \"X\":\n",
    "#             white = net; break\n",
    "#         else:\n",
    "#             print(\"I didn't get that.\")\n",
    "#     current_board = board()\n",
    "#     checkmate = False\n",
    "#     dataset = []\n",
    "#     value = 0; t = 0.1; moves_count = 0\n",
    "#     while checkmate == False and current_board.actions() != []:\n",
    "#         if moves_count <= 5:\n",
    "#             t = 1\n",
    "#         else:\n",
    "#             t = 0.1\n",
    "#         moves_count += 1\n",
    "#         dataset.append(copy.deepcopy(encode_board(current_board)))\n",
    "#         print(current_board.current_board); print(\" \")\n",
    "#         if current_board.player == 0:\n",
    "#             if white != None:\n",
    "#                 print(\"AI is thinking........\")\n",
    "#                 root = UCT_search(current_board,777,white,t)\n",
    "#                 policy = get_policy(root, t)\n",
    "#             else:\n",
    "#                 while(True):\n",
    "#                     col = input(\"Which column do you wanna drop your piece? (Enter 1-7)\\n\")\n",
    "#                     if int(col) in [1,2,3,4,5,6,7]:\n",
    "#                         policy = np.zeros([7], dtype=np.float32); policy[int(col)-1] += 1\n",
    "#                         break\n",
    "#         elif current_board.player == 1:\n",
    "#             if black != None:\n",
    "#                 print(\"AI is thinking.............\")\n",
    "#                 root = UCT_search(current_board,777,black,t)\n",
    "#                 policy = get_policy(root, t)\n",
    "#             else:\n",
    "#                 while(True):\n",
    "#                     col = input(\"Which column do you wanna drop your piece? (Enter 1-7)\\n\")\n",
    "#                     if int(col) in [1,2,3,4,5,6,7]:\n",
    "#                         policy = np.zeros([7], dtype=np.float32); policy[int(col)-1] += 1\n",
    "#                         break\n",
    "#         current_board = do_decode_n_move_pieces(current_board,\\\n",
    "#                                                 np.random.choice(np.array([0,1,2,3,4,5,6]), \\\n",
    "#                                                                  p = policy)) # decode move and move piece(s)\n",
    "#         if current_board.check_winner() == True: # someone wins\n",
    "#             if current_board.player == 0: # black wins\n",
    "#                 value = -1\n",
    "#             elif current_board.player == 1: # white wins\n",
    "#                 value = 1\n",
    "#             checkmate = True\n",
    "#     dataset.append(encode_board(current_board))\n",
    "#     print(current_board.current_board); print(\" \")\n",
    "#     if value == -1:\n",
    "#         if play_as == \"O\":\n",
    "#             dataset.append(f\"AI as black wins\"); print(\"YOU LOSE!!!!!!!\")\n",
    "#         else:\n",
    "#             dataset.append(f\"Human as black wins\"); print(\"YOU WIN!!!!!!!\")\n",
    "#         return \"black\", dataset\n",
    "#     elif value == 1:\n",
    "#         if play_as == \"O\":\n",
    "#             dataset.append(f\"Human as white wins\"); print(\"YOU WIN!!!!!!!!!!!\")\n",
    "#         else:\n",
    "#             dataset.append(f\"AI as white wins\"); print(\"YOU LOSE!!!!!!!\")\n",
    "#         return \"white\", dataset\n",
    "#     else:\n",
    "#         dataset.append(\"Nobody wins\"); print(\"DRAW!!!!!\")\n",
    "#         return None, dataset\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     best_net=\"c4_current_net_trained1_iter6.pth.tar\"\n",
    "#     best_net_filename = os.path.join(\"./model_data/\",\\\n",
    "#                                     best_net)\n",
    "#     best_cnet = ConnectNet()\n",
    "#     cuda = torch.cuda.is_available()\n",
    "#     if cuda:\n",
    "#         best_cnet.cuda()\n",
    "#     best_cnet.eval()\n",
    "#     checkpoint = torch.load(best_net_filename)\n",
    "#     best_cnet.load_state_dict(checkpoint['state_dict'])\n",
    "#     play_again = True\n",
    "#     while(play_again == True):\n",
    "#         play_game(best_cnet)\n",
    "#         while(True):\n",
    "#             again = input(\"Do you wanna play again? (Y/N)\\n\")\n",
    "#             if again.lower() in [\"y\", \"n\"]:\n",
    "#                 if again.lower() == \"n\":\n",
    "#                     play_again = False; break\n",
    "#                 else:\n",
    "#                     break\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state(net, optimizer, scheduler, args, iteration, new_optim_state=True):\n",
    "    \"\"\" Loads saved model and optimizer states if exists \"\"\"\n",
    "    base_path = \"./model_data/\"\n",
    "    checkpoint_path = os.path.join(base_path, \"%s_iter%d.pth.tar\" % (args.neural_net_name, iteration))\n",
    "    start_epoch, checkpoint = 0, None\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    if checkpoint != None:\n",
    "        if (len(checkpoint) == 1) or (new_optim_state == True):\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "        else:\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    return start_epoch\n",
    "\n",
    "\n",
    "def load_results(iteration):\n",
    "    \"\"\" Loads saved results if exists \"\"\"\n",
    "    losses_path = \"./model_data/losses_per_epoch_iter%d.pkl\" % iteration\n",
    "    if os.path.isfile(losses_path):\n",
    "        losses_per_epoch = load_pickle(\"losses_per_epoch_iter%d.pkl\" % iteration)\n",
    "    else:\n",
    "        losses_per_epoch = []\n",
    "    return losses_per_epoch\n",
    "\n",
    "\n",
    "def train(net, dataset, optimizer, scheduler, start_epoch, cpu, args, iteration):\n",
    "    torch.manual_seed(cpu)\n",
    "    cuda = torch.cuda.is_available()\n",
    "    net.train()\n",
    "    criterion = AlphaLoss()\n",
    "    \n",
    "    train_set = board_data(dataset)\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
    "    losses_per_epoch = load_results(iteration + 1)\n",
    "    \n",
    "    update_size = len(train_loader)//10\n",
    "    print(\"Update step size: %d\" % update_size)\n",
    "    for epoch in range(start_epoch, args.num_epochs):\n",
    "        total_loss = 0.0\n",
    "        losses_per_batch = []\n",
    "        for i,data in enumerate(train_loader,0):\n",
    "            state, policy, value = data\n",
    "            state, policy, value = state.float(), policy.float(), value.float()\n",
    "            if cuda:\n",
    "                state, policy, value = state.cuda(), policy.cuda(), value.cuda()\n",
    "            policy_pred, value_pred = net(state) # policy_pred = torch.Size([batch, 4672]) value_pred = torch.Size([batch, 1])\n",
    "            loss = criterion(value_pred[:,0], value, policy_pred, policy)\n",
    "            loss = loss/args.gradient_acc_steps\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(net.parameters(), args.max_norm)\n",
    "            if (epoch % args.gradient_acc_steps) == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "            if i % update_size == (update_size - 1):    # print every update_size-d mini-batches of size = batch_size\n",
    "                losses_per_batch.append(args.gradient_acc_steps*total_loss/update_size)\n",
    "                print('[Iteration %d] Process ID: %d [Epoch: %d, %5d/ %d points] total loss per batch: %.3f' %\n",
    "                      (iteration, os.getpid(), epoch + 1, (i + 1)*args.batch_size, len(train_set), losses_per_batch[-1]))\n",
    "                print(\"Policy (actual, predicted):\",policy[0].argmax().item(),policy_pred[0].argmax().item())\n",
    "                print(\"Policy data:\", policy[0]); print(\"Policy pred:\", policy_pred[0])\n",
    "                print(\"Value (actual, predicted):\", value[0].item(), value_pred[0,0].item())\n",
    "                #print(\"Conv grad: %.7f\" % net.conv.conv1.weight.grad.mean().item())\n",
    "                #print(\"Res18 grad %.7f:\" % net.res_18.conv1.weight.grad.mean().item())\n",
    "                print(\" \")\n",
    "                total_loss = 0.0\n",
    "        \n",
    "        scheduler.step()\n",
    "        if len(losses_per_batch) >= 1:\n",
    "            losses_per_epoch.append(sum(losses_per_batch)/len(losses_per_batch))\n",
    "        if (epoch % 2) == 0:\n",
    "            save_as_pickle(\"losses_per_epoch_iter%d.pkl\" % (iteration + 1), losses_per_epoch)\n",
    "            torch.save({\n",
    "                    'epoch': epoch + 1,\\\n",
    "                    'state_dict': net.state_dict(),\\\n",
    "                    'optimizer' : optimizer.state_dict(),\\\n",
    "                    'scheduler' : scheduler.state_dict(),\\\n",
    "                }, os.path.join(\"./model_data/\",\\\n",
    "                    \"%s_iter%d.pth.tar\" % (args.neural_net_name, (iteration + 1))))\n",
    "        '''\n",
    "        # Early stopping\n",
    "        if len(losses_per_epoch) > 50:\n",
    "            if abs(sum(losses_per_epoch[-4:-1])/3-sum(losses_per_epoch[-16:-13])/3) <= 0.00017:\n",
    "                break\n",
    "        '''\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(222)\n",
    "    ax.scatter([e for e in range(start_epoch, (len(losses_per_epoch) + start_epoch))], losses_per_epoch)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss per batch\")\n",
    "    ax.set_title(\"Loss vs Epoch\")\n",
    "    plt.savefig(os.path.join(\"./model_data/\", \"Loss_vs_Epoch_iter%d_%s.png\" % ((iteration + 1), datetime.datetime.today().strftime(\"%Y-%m-%d\"))))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def train_connectnet(args, iteration, new_optim_state):\n",
    "    # gather data\n",
    "    data_path=\"./datasets/iter_%d/\" % iteration\n",
    "    datasets = []\n",
    "    for idx,file in enumerate(os.listdir(data_path)):\n",
    "        filename = os.path.join(data_path,file)\n",
    "        with open(filename, 'rb') as fo:\n",
    "            datasets.extend(pickle.load(fo, encoding='bytes'))\n",
    "    datasets = np.array(datasets)\n",
    "    \n",
    "    # train net\n",
    "    net = ConnectNet()\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=args.lr, betas=(0.8, 0.999))\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,100,150,200,250,300,400], gamma=0.77)\n",
    "    start_epoch = load_state(net, optimizer, scheduler, args, iteration, new_optim_state)\n",
    "    \n",
    "    train(net, datasets, optimizer, scheduler, start_epoch, 0, args, iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--iteration\", type=int, default=0, help=\"Current iteration number to resume from\")\n",
    "parser.add_argument(\"--total_iterations\", type=int, default=1000, help=\"Total number of iterations to run\")\n",
    "parser.add_argument(\"--MCTS_num_processes\", type=int, default=5, help=\"Number of processes to run MCTS self-plays\")\n",
    "parser.add_argument(\"--num_games_per_MCTS_process\", type=int, default=120, help=\"Number of games to simulate per MCTS self-play process\")\n",
    "parser.add_argument(\"--temperature_MCTS\", type=float, default=1.1, help=\"Temperature for first 10 moves of each MCTS self-play\")\n",
    "parser.add_argument(\"--num_evaluator_games\", type=int, default=100, help=\"No of games to play to evaluate neural nets\")\n",
    "parser.add_argument(\"--neural_net_name\", type=str, default=\"cc4_current_net_\", help=\"Name of neural net\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Training batch size\")\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=300, help=\"No of epochs\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")\n",
    "parser.add_argument(\"--gradient_acc_steps\", type=int, default=1, help=\"Number of steps of gradient accumulation\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=1.0, help=\"Clipped gradient norm\")\n",
    "args = parser.parse_args('')\n",
    "\n",
    "for i in range(args.iteration, args.total_iterations): \n",
    "    run_MCTS(args, start_idx=0, iteration=i)\n",
    "    train_connectnet(args, iteration=i, new_optim_state=True)\n",
    "    if i >= 1:\n",
    "        winner = evaluate_nets(args, i, i + 1)\n",
    "        counts = 0\n",
    "        while (winner != (i + 1)):\n",
    "            run_MCTS(args, start_idx=(counts + 1)*args.num_games_per_MCTS_process, iteration=i)\n",
    "            counts += 1\n",
    "            train_connectnet(args, iteration=i, new_optim_state=True)\n",
    "            winner = evaluate_nets(args, i, i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources / References\n",
    "\n",
    "## Game Theory\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Game_theory\n",
    "* https://en.wikipedia.org/wiki/Complete_information\n",
    "* https://en.wikipedia.org/wiki/Perfect_information\n",
    "\n",
    "## Residual Networks\n",
    "\n",
    "* https://towardsdatascience.com/residual-network-implementing-resnet-a7da63c7b278\n",
    "\n",
    "## Monte-Carlo Tree Search\n",
    "\n",
    "* https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\n",
    "\n",
    "## Upper Confidence bounds applied to Trees\n",
    "\n",
    "* https://www.chessprogramming.org/UCT\n",
    "\n",
    "## Various RL Algorithms (in PyTorch)\n",
    "\n",
    "* https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch\n",
    "\n",
    "## Curated Resources on RL\n",
    "\n",
    "* https://github.com/aikorea/awesome-rl\n",
    "* https://github.com/tigerneil/awesome-deep-rl\n",
    "\n",
    "## AlphaGo\n",
    "\n",
    "* https://en.wikipedia.org/wiki/AlphaGo\n",
    "\n",
    "## AlphaZero\n",
    "\n",
    "* https://en.wikipedia.org/wiki/AlphaZero\n",
    "\n",
    "## Inverse Reinforcement Learning (in PyTorch)\n",
    "\n",
    "* https://github.com/reinforcement-learning-kr/lets-do-irl\n",
    "\n",
    "## Flappy Bird RL Tutorial (in PyTorch)\n",
    "\n",
    "* https://www.toptal.com/deep-learning/pytorch-reinforcement-learning-tutorial\n",
    "\n",
    "## AlphaZero for Connect 4 (in PyTorch)\n",
    "\n",
    "* https://towardsdatascience.com/from-scratch-implementation-of-alphazero-for-connect4-f73d4554002a\n",
    "\n",
    "## Collection of Optimizers for PyTorch\n",
    "\n",
    "* https://github.com/jettify/pytorch-optimizer\n",
    "\n",
    "# Further Reading\n",
    "\n",
    "## Solving Games with Imperfect Information\n",
    "\n",
    "* https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/viewFile/8407/8476\n",
    "* https://en.wikipedia.org/wiki/Libratus\n",
    "* https://www.youtube.com/watch?v=2dX0lwaQRX0\n",
    "* https://arxiv.org/pdf/1811.00164.pdf\n",
    "\n",
    "## Counter-Factual Regret Minimization (for Poker) in Python\n",
    "\n",
    "* https://github.com/tansey/pycfr\n",
    "\n",
    "## Inverse Reinforcement Learning\n",
    "\n",
    "* https://ai.stanford.edu/~ang/papers/icml00-irl.pdf\n",
    "* https://arxiv.org/pdf/1806.06877.pdf\n",
    "* https://en.wikipedia.org/wiki/Apprenticeship_learning\n",
    "* https://en.wikipedia.org/wiki/Reinforcement_learning#Inverse_reinforcement_learning\n",
    "* https://jangirrishabh.github.io/2016/07/09/virtual-car-IRL/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
